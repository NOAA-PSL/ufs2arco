{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8566fedf-adb5-4105-99f1-78e257348ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "\n",
    "import subprocess\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ae5706-3ba8-43df-874e-c7e1aefef00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ufs2arco import sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b5917a0-9496-4d09-8676-f63cffd71497",
   "metadata": {},
   "outputs": [],
   "source": [
    "gfs = sources.RDAGFSArchive(\n",
    "    t0={\"start\": \"2015-12-31T00\", \"end\": \"2024-12-31T00\", \"freq\": \"1YE\"},\n",
    "    fhr={\"start\": 0, \"end\": 6, \"step\": 6},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0742def-cb0d-4093-9430-3dd82f61920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_both_local(dims, cache_dir):\n",
    "    a = gfs._open_local(\n",
    "        dims=dims,\n",
    "        file_suffix=\"\",\n",
    "        cache_dir=\"./gribcache\",\n",
    "    )\n",
    "    b = gfs._open_local(\n",
    "        dims=dims,\n",
    "        file_suffix=\"b\",\n",
    "        cache_dir=\"./gribcache\",\n",
    "    )\n",
    "    return a,b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deb13eac-b176-44fd-9841-4563eb7f616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_datasets(dims, cache_dir, **kwargs):\n",
    "\n",
    "    dsdict = {}\n",
    "    for file_suffix in [\"\", \"b\"]:\n",
    "        try:\n",
    "            dsdict[file_suffix] = gfs.open_grib(\n",
    "                dims=dims,\n",
    "                file_suffix=file_suffix,\n",
    "                cache_dir=\"./gribcache\",\n",
    "                **kwargs,\n",
    "            ) \n",
    "        except:\n",
    "            dsdict[file_suffix] = None\n",
    "\n",
    "    if all([xx is not None for xx in dsdict.values()]):\n",
    "        alist = set(sorted(dsdict[\"\"].data_vars))\n",
    "        blist = set(sorted(dsdict[\"b\"].data_vars))\n",
    "        xds = xr.merge(list(dsdict.values()))\n",
    "        \n",
    "    elif dsdict[\"\"] is not None or dsdict[\"b\"] is not None:\n",
    "        if dsdict[\"\"] is not None:\n",
    "            alist = set(sorted(dsdict[\"\"].data_vars))\n",
    "            blist = set()\n",
    "            xds = dsdict[\"\"]\n",
    "        else:\n",
    "            alist = set()\n",
    "            blist = set(sorted(dsdict[\"b\"].data_vars))\n",
    "            xds = dsdict[\"b\"]\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "    varlist = set(sorted(xds.data_vars))\n",
    "    onlyA = alist - varlist.intersection(blist)\n",
    "    onlyB = blist - varlist.intersection(alist)\n",
    "    for key in varlist:\n",
    "        if key in onlyA:\n",
    "            xds[key].attrs[\"file_suffix\"] = [\"\"]\n",
    "        elif key in onlyB:\n",
    "            xds[key].attrs[\"file_suffix\"] = [\"b\"]\n",
    "        else:\n",
    "            xds[key].attrs[\"file_suffix\"] = [\"\", \"b\"]\n",
    "    return xds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ca190f-eddc-4fcb-90a2-4a3f463d9d1d",
   "metadata": {},
   "source": [
    "### First, figure out levels available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd95f848-03c6-499a-a91d-238885d6559f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading (t0, fhr) = (2015-12-31 00:00:00, 0)\n",
      "Reading (t0, fhr) = (2015-12-31 00:00:00, 6)\n",
      "Reading (t0, fhr) = (2016-12-31 00:00:00, 0)\n",
      "Reading (t0, fhr) = (2016-12-31 00:00:00, 6)\n",
      "Reading (t0, fhr) = (2017-12-31 00:00:00, 0)\n",
      "Reading (t0, fhr) = (2017-12-31 00:00:00, 6)\n",
      "Reading (t0, fhr) = (2018-12-31 00:00:00, 0)\n",
      "Reading (t0, fhr) = (2018-12-31 00:00:00, 6)\n",
      "Reading (t0, fhr) = (2019-12-31 00:00:00, 0)\n",
      "Reading (t0, fhr) = (2019-12-31 00:00:00, 6)\n",
      "Reading (t0, fhr) = (2020-12-31 00:00:00, 0)\n",
      "Reading (t0, fhr) = (2020-12-31 00:00:00, 6)\n",
      "Reading (t0, fhr) = (2021-12-31 00:00:00, 0)\n",
      "Reading (t0, fhr) = (2021-12-31 00:00:00, 6)\n",
      "Reading (t0, fhr) = (2022-12-31 00:00:00, 0)\n",
      "Reading (t0, fhr) = (2022-12-31 00:00:00, 6)\n",
      "Reading (t0, fhr) = (2023-12-31 00:00:00, 0)\n",
      "Reading (t0, fhr) = (2023-12-31 00:00:00, 6)\n",
      "Reading (t0, fhr) = (2024-12-31 00:00:00, 0)\n",
      "Reading (t0, fhr) = (2024-12-31 00:00:00, 6)\n"
     ]
    }
   ],
   "source": [
    "dsdict = {}\n",
    "for t0 in gfs.t0:\n",
    "    dsdict[t0] = {}\n",
    "\n",
    "    for fhr in gfs.fhr:\n",
    "        print(f\"Reading (t0, fhr) = ({str(t0)}, {int(fhr)})\")\n",
    "        a, b = pull_both_local(\n",
    "            dims={\"t0\": t0, \"fhr\": fhr},\n",
    "            cache_dir=\"./gribcache\",\n",
    "        )\n",
    "        levels = []\n",
    "        for file in [a, b]:\n",
    "            output = subprocess.check_output(\n",
    "                [\"grib_ls\", \"-p\", \"level,typeOfLevel\", a],\n",
    "                stderr=subprocess.DEVNULL\n",
    "            ).decode()\n",
    "\n",
    "            for line in output.splitlines():\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    try:\n",
    "                        level, type_of_level = int(parts[-2]), parts[-1]\n",
    "                        if type_of_level == \"heightAboveGround\":\n",
    "                            levels.append(level)\n",
    "                    except:\n",
    "                        continue\n",
    "        dsdict[t0][fhr] = sorted(set(levels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "175ea89b-5c6e-48d6-9281-e57fb64c22be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t0 = 2015-12-31 00:00:00\n",
      "\t[2, 10, 80, 100] \t [2, 10, 80, 100]\n",
      "t0 = 2016-12-31 00:00:00\n",
      "\t[2, 10, 80, 100] \t [2, 10, 80, 100]\n",
      "t0 = 2017-12-31 00:00:00\n",
      "\t[2, 10, 80, 100] \t [2, 10, 80, 100]\n",
      "t0 = 2018-12-31 00:00:00\n",
      "\t[2, 10, 80, 100] \t [2, 10, 80, 100]\n",
      "t0 = 2019-12-31 00:00:00\n",
      "\t[2, 10, 20, 30, 40, 50, 80, 100] \t [2, 10, 20, 30, 40, 50, 80, 100]\n",
      "t0 = 2020-12-31 00:00:00\n",
      "\t[2, 10, 20, 30, 40, 50, 80, 100] \t [2, 10, 20, 30, 40, 50, 80, 100]\n",
      "t0 = 2021-12-31 00:00:00\n",
      "\t[2, 10, 20, 30, 40, 50, 80, 100, 1000, 4000] \t [2, 10, 20, 30, 40, 50, 80, 100, 1000, 4000]\n",
      "t0 = 2022-12-31 00:00:00\n",
      "\t[2, 10, 20, 30, 40, 50, 80, 100, 1000, 4000] \t [2, 10, 20, 30, 40, 50, 80, 100, 1000, 4000]\n",
      "t0 = 2023-12-31 00:00:00\n",
      "\t[2, 10, 20, 30, 40, 50, 80, 100, 1000, 4000] \t [2, 10, 20, 30, 40, 50, 80, 100, 1000, 4000]\n",
      "t0 = 2024-12-31 00:00:00\n",
      "\t[2, 10, 20, 30, 40, 50, 80, 100, 1000, 4000] \t [2, 10, 20, 30, 40, 50, 80, 100, 1000, 4000]\n"
     ]
    }
   ],
   "source": [
    "for t0, fdict in dsdict.items():\n",
    "    print(f\"t0 = {t0}\")\n",
    "    print(f\"\\t{fdict[0]} \\t {fdict[6]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad70d5-a022-439b-85ab-1edb2f829c9a",
   "metadata": {},
   "source": [
    "So the common levels are `[2, 10, 80, 100]`\n",
    "\n",
    "But there are more start sometime in 2019, and many more in 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33e4445-7fb5-4dc6-954e-b39689bb0c32",
   "metadata": {},
   "source": [
    "### Now, get the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d830ed11-9704-46a0-b96c-5e9dfe0d7a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [2, 10, 80, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "47b6f9a8-0ace-47f9-a529-343768df88de",
   "metadata": {},
   "outputs": [],
   "source": [
    "vdict = {}\n",
    "for level in levels:\n",
    "    vdict[level] = dict()\n",
    "    for t0 in gfs.t0:\n",
    "        vdict[level][t0] = {}\n",
    "        dslist = []\n",
    "        varlist = []\n",
    "        for fhr in gfs.fhr:\n",
    "            xds = open_datasets(\n",
    "                dims={\"t0\": t0, \"fhr\": fhr},\n",
    "                cache_dir=\"./gribcache\",\n",
    "                filter_by_keys={\n",
    "                    \"typeOfLevel\": \"heightAboveGround\",\n",
    "                    \"level\": level,\n",
    "                },\n",
    "            )\n",
    "            vdict[level][t0][fhr] = set(xds.data_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0679dfc-8cf7-4769-ae44-1662245aaf8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: {Timestamp('2015-12-31 00:00:00'): {np.int64(0): {'aptmp',\n",
       "    'd2m',\n",
       "    'r2',\n",
       "    'sh2',\n",
       "    't2m'},\n",
       "   np.int64(6): {'aptmp', 'd2m', 'r2', 'sh2', 't2m', 'tmax', 'tmin'}},\n",
       "  Timestamp('2016-12-31 00:00:00'): {np.int64(0): {'aptmp',\n",
       "    'd2m',\n",
       "    'r2',\n",
       "    'sh2',\n",
       "    't2m'},\n",
       "   np.int64(6): {'aptmp', 'd2m', 'r2', 'sh2', 't2m', 'tmax', 'tmin'}},\n",
       "  Timestamp('2017-12-31 00:00:00'): {np.int64(0): {'aptmp',\n",
       "    'd2m',\n",
       "    'r2',\n",
       "    'sh2',\n",
       "    't2m'},\n",
       "   np.int64(6): {'aptmp', 'd2m', 'r2', 'sh2', 't2m', 'tmax', 'tmin'}},\n",
       "  Timestamp('2018-12-31 00:00:00'): {np.int64(0): {'aptmp',\n",
       "    'd2m',\n",
       "    'r2',\n",
       "    'sh2',\n",
       "    't2m'},\n",
       "   np.int64(6): {'aptmp', 'd2m', 'r2', 'sh2', 't2m', 'tmax', 'tmin'}},\n",
       "  Timestamp('2019-12-31 00:00:00'): {np.int64(0): {'aptmp',\n",
       "    'd2m',\n",
       "    'r2',\n",
       "    'sh2',\n",
       "    't2m'},\n",
       "   np.int64(6): {'aptmp', 'd2m', 'r2', 'sh2', 't2m', 'tmax', 'tmin'}},\n",
       "  Timestamp('2020-12-31 00:00:00'): {np.int64(0): {'aptmp',\n",
       "    'd2m',\n",
       "    'r2',\n",
       "    'sh2',\n",
       "    't2m'},\n",
       "   np.int64(6): {'aptmp', 'd2m', 'r2', 'sh2', 't2m', 'tmax', 'tmin'}},\n",
       "  Timestamp('2021-12-31 00:00:00'): {np.int64(0): {'aptmp',\n",
       "    'd2m',\n",
       "    'r2',\n",
       "    'sh2',\n",
       "    't2m'},\n",
       "   np.int64(6): {'aptmp', 'd2m', 'r2', 'sh2', 't2m', 'tmax', 'tmin'}},\n",
       "  Timestamp('2022-12-31 00:00:00'): {np.int64(0): {'aptmp',\n",
       "    'd2m',\n",
       "    'r2',\n",
       "    'sh2',\n",
       "    't2m'},\n",
       "   np.int64(6): {'aptmp', 'd2m', 'r2', 'sh2', 't2m', 'tmax', 'tmin'}},\n",
       "  Timestamp('2023-12-31 00:00:00'): {np.int64(0): {'aptmp',\n",
       "    'd2m',\n",
       "    'r2',\n",
       "    'sh2',\n",
       "    't2m'},\n",
       "   np.int64(6): {'aptmp', 'd2m', 'r2', 'sh2', 't2m', 'tmax', 'tmin'}},\n",
       "  Timestamp('2024-12-31 00:00:00'): {np.int64(0): {'aptmp',\n",
       "    'd2m',\n",
       "    'r2',\n",
       "    'sh2',\n",
       "    't2m'},\n",
       "   np.int64(6): {'aptmp', 'd2m', 'r2', 'sh2', 't2m', 'tmax', 'tmin'}}},\n",
       " 10: {Timestamp('2015-12-31 00:00:00'): {np.int64(0): {'u10', 'v10'},\n",
       "   np.int64(6): {'u10', 'v10'}},\n",
       "  Timestamp('2016-12-31 00:00:00'): {np.int64(0): {'u10', 'v10'},\n",
       "   np.int64(6): {'u10', 'v10'}},\n",
       "  Timestamp('2017-12-31 00:00:00'): {np.int64(0): {'u10', 'v10'},\n",
       "   np.int64(6): {'u10', 'v10'}},\n",
       "  Timestamp('2018-12-31 00:00:00'): {np.int64(0): {'u10', 'v10'},\n",
       "   np.int64(6): {'u10', 'v10'}},\n",
       "  Timestamp('2019-12-31 00:00:00'): {np.int64(0): {'u10', 'v10'},\n",
       "   np.int64(6): {'u10', 'v10'}},\n",
       "  Timestamp('2020-12-31 00:00:00'): {np.int64(0): {'u10', 'v10'},\n",
       "   np.int64(6): {'u10', 'v10'}},\n",
       "  Timestamp('2021-12-31 00:00:00'): {np.int64(0): {'u10', 'v10'},\n",
       "   np.int64(6): {'u10', 'v10'}},\n",
       "  Timestamp('2022-12-31 00:00:00'): {np.int64(0): {'u10', 'v10'},\n",
       "   np.int64(6): {'u10', 'v10'}},\n",
       "  Timestamp('2023-12-31 00:00:00'): {np.int64(0): {'u10', 'v10'},\n",
       "   np.int64(6): {'u10', 'v10'}},\n",
       "  Timestamp('2024-12-31 00:00:00'): {np.int64(0): {'u10', 'v10'},\n",
       "   np.int64(6): {'u10', 'v10'}}},\n",
       " 80: {Timestamp('2015-12-31 00:00:00'): {np.int64(0): {'pres',\n",
       "    'q',\n",
       "    't',\n",
       "    'u',\n",
       "    'v'},\n",
       "   np.int64(6): {'pres', 'q', 't', 'u', 'v'}},\n",
       "  Timestamp('2016-12-31 00:00:00'): {np.int64(0): {'pres', 'q', 't', 'u', 'v'},\n",
       "   np.int64(6): {'pres', 'q', 't', 'u', 'v'}},\n",
       "  Timestamp('2017-12-31 00:00:00'): {np.int64(0): {'pres', 'q', 't', 'u', 'v'},\n",
       "   np.int64(6): {'pres', 'q', 't', 'u', 'v'}},\n",
       "  Timestamp('2018-12-31 00:00:00'): {np.int64(0): {'pres', 'q', 't', 'u', 'v'},\n",
       "   np.int64(6): {'pres', 'q', 't', 'u', 'v'}},\n",
       "  Timestamp('2019-12-31 00:00:00'): {np.int64(0): {'pres', 'q', 't', 'u', 'v'},\n",
       "   np.int64(6): {'pres', 'q', 't', 'u', 'v'}},\n",
       "  Timestamp('2020-12-31 00:00:00'): {np.int64(0): {'pres', 'q', 't', 'u', 'v'},\n",
       "   np.int64(6): {'pres', 'q', 't', 'u', 'v'}},\n",
       "  Timestamp('2021-12-31 00:00:00'): {np.int64(0): {'pres', 'q', 't', 'u', 'v'},\n",
       "   np.int64(6): {'pres', 'q', 't', 'u', 'v'}},\n",
       "  Timestamp('2022-12-31 00:00:00'): {np.int64(0): {'pres', 'q', 't', 'u', 'v'},\n",
       "   np.int64(6): {'pres', 'q', 't', 'u', 'v'}},\n",
       "  Timestamp('2023-12-31 00:00:00'): {np.int64(0): {'pres', 'q', 't', 'u', 'v'},\n",
       "   np.int64(6): {'pres', 'q', 't', 'u', 'v'}},\n",
       "  Timestamp('2024-12-31 00:00:00'): {np.int64(0): {'pres', 'q', 't', 'u', 'v'},\n",
       "   np.int64(6): {'pres', 'q', 't', 'u', 'v'}}},\n",
       " 100: {Timestamp('2015-12-31 00:00:00'): {np.int64(0): {'t', 'u100', 'v100'},\n",
       "   np.int64(6): {'t', 'u100', 'v100'}},\n",
       "  Timestamp('2016-12-31 00:00:00'): {np.int64(0): {'t', 'u100', 'v100'},\n",
       "   np.int64(6): {'t', 'u100', 'v100'}},\n",
       "  Timestamp('2017-12-31 00:00:00'): {np.int64(0): {'t', 'u100', 'v100'},\n",
       "   np.int64(6): {'t', 'u100', 'v100'}},\n",
       "  Timestamp('2018-12-31 00:00:00'): {np.int64(0): {'t', 'u100', 'v100'},\n",
       "   np.int64(6): {'t', 'u100', 'v100'}},\n",
       "  Timestamp('2019-12-31 00:00:00'): {np.int64(0): {'t', 'u100', 'v100'},\n",
       "   np.int64(6): {'t', 'u100', 'v100'}},\n",
       "  Timestamp('2020-12-31 00:00:00'): {np.int64(0): {'t', 'u100', 'v100'},\n",
       "   np.int64(6): {'t', 'u100', 'v100'}},\n",
       "  Timestamp('2021-12-31 00:00:00'): {np.int64(0): {'t', 'u100', 'v100'},\n",
       "   np.int64(6): {'t', 'u100', 'v100'}},\n",
       "  Timestamp('2022-12-31 00:00:00'): {np.int64(0): {'t', 'u100', 'v100'},\n",
       "   np.int64(6): {'t', 'u100', 'v100'}},\n",
       "  Timestamp('2023-12-31 00:00:00'): {np.int64(0): {'t', 'u100', 'v100'},\n",
       "   np.int64(6): {'t', 'u100', 'v100'}},\n",
       "  Timestamp('2024-12-31 00:00:00'): {np.int64(0): {'t', 'u100', 'v100'},\n",
       "   np.int64(6): {'t', 'u100', 'v100'}}}}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "acf4705e-082a-44a5-808a-7e5dd0fb538e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "More in forecast t0 = 2015-12-31 00:00:00, level = 2\n",
      "\t{'tmin', 'tmax'}\n",
      "More in forecast t0 = 2016-12-31 00:00:00, level = 2\n",
      "\t{'tmin', 'tmax'}\n",
      "More in forecast t0 = 2017-12-31 00:00:00, level = 2\n",
      "\t{'tmin', 'tmax'}\n",
      "More in forecast t0 = 2018-12-31 00:00:00, level = 2\n",
      "\t{'tmin', 'tmax'}\n",
      "More in forecast t0 = 2019-12-31 00:00:00, level = 2\n",
      "\t{'tmin', 'tmax'}\n",
      "More in forecast t0 = 2020-12-31 00:00:00, level = 2\n",
      "\t{'tmin', 'tmax'}\n",
      "More in forecast t0 = 2021-12-31 00:00:00, level = 2\n",
      "\t{'tmin', 'tmax'}\n",
      "More in forecast t0 = 2022-12-31 00:00:00, level = 2\n",
      "\t{'tmin', 'tmax'}\n",
      "More in forecast t0 = 2023-12-31 00:00:00, level = 2\n",
      "\t{'tmin', 'tmax'}\n",
      "More in forecast t0 = 2024-12-31 00:00:00, level = 2\n",
      "\t{'tmin', 'tmax'}\n"
     ]
    }
   ],
   "source": [
    "for level, d2 in vdict.items():\n",
    "    for t0, d3 in d2.items():\n",
    "        intersect = reduce(set.intersection, [set(x) for x in d3.values()]) \n",
    "        analysis_only = d3[0] - intersect\n",
    "        if len(d3[0] - intersect) > 0:\n",
    "            print(f\"More in analysis t0 = {t0}, level = {level}\")\n",
    "            print(f\"\\t{analysis_only}\")\n",
    "        forecast_only = d3[6] - intersect\n",
    "        if len(forecast_only) > 0:\n",
    "            print(f\"More in forecast t0 = {t0}, level = {level}\")\n",
    "            print(f\"\\t{forecast_only}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0350e1-b3f3-4331-ab9b-4e94a2946edf",
   "metadata": {},
   "source": [
    "forecast has the additional variables `tmax` and `tmin`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303314ae-ce2b-4185-b893-202c28d1f70e",
   "metadata": {},
   "source": [
    "### Get the common variables in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7304d89c-4abc-460e-a905-e1553c6fc831",
   "metadata": {},
   "outputs": [],
   "source": [
    "intersect = {}\n",
    "intersect[0] = {\n",
    "    key: sorted(reduce(set.intersection, [set(x[0]) for x in vdict[key].values()]))\n",
    "    for key in vdict.keys()\n",
    "}\n",
    "\n",
    "intersect[6] = {\n",
    "    key: sorted(reduce(set.intersection, [set(x[6]) for x in vdict[key].values()]))\n",
    "    for key in vdict.keys()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb290b07-4f6d-4a30-8f14-63a19f8b32e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: ['aptmp', 'd2m', 'r2', 'sh2', 't2m'],\n",
       " 10: ['u10', 'v10'],\n",
       " 80: ['pres', 'q', 't', 'u', 'v'],\n",
       " 100: ['t', 'u100', 'v100']}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersect[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "255138f0-05a3-4140-8f99-93e298f5a716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: ['aptmp', 'd2m', 'r2', 'sh2', 't2m', 'tmax', 'tmin'],\n",
       " 10: ['u10', 'v10'],\n",
       " 80: ['pres', 'q', 't', 'u', 'v'],\n",
       " 100: ['t', 'u100', 'v100']}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersect[6]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160b3ae6-a843-4f81-845b-2cc97eade87d",
   "metadata": {},
   "source": [
    "### Get the unique per t0 variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b2222fd5-89a3-482c-a228-874697548dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level = 2\n",
      "level = 10\n",
      "level = 80\n",
      "level = 100\n"
     ]
    }
   ],
   "source": [
    "for level, d2 in vdict.items():\n",
    "    print(f\"level = {level}\")\n",
    "    for t0, d3 in d2.items():\n",
    "        for fhr, d4 in d3.items():\n",
    "            unique = set(d4) - set(intersect[fhr][level])\n",
    "            if len(unique) > 0:\n",
    "                print(f\"\\t{t0} fhr = {fhr}\")\n",
    "                print(f\"\\t\\t{unique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a61bcaa-5287-43b2-82fb-e9d52fc926e4",
   "metadata": {},
   "source": [
    "WOW! for once, the variables are the same, at least among the commonly available levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9398a283-1bfe-456f-8c3f-b4bfcb1df931",
   "metadata": {},
   "source": [
    "### Now, let's open a dataset, get these variables, and write out an updated dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d924cb28-d3e4-45c4-b7b2-b8b67770091d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsdict = {}\n",
    "for level in levels:\n",
    "    xds = open_datasets(\n",
    "        dims={\"t0\": gfs.t0[0], \"fhr\": gfs.fhr[1]},\n",
    "        cache_dir=\"./gribcache\",\n",
    "        filter_by_keys={\n",
    "            \"typeOfLevel\": \"heightAboveGround\",\n",
    "            \"level\": level,\n",
    "        },\n",
    "    )\n",
    "    xds = xds[sorted(intersect[6][level])]\n",
    "    if \"unknown\" in xds:\n",
    "        xds = xds.drop_vars(\"unknown\")\n",
    "\n",
    "    for key in [\"aptmp\", \"tmax\", \"tmin\", \"pres\", \"t\", \"q\", \"u\", \"v\", \"pt\", \"refd\"]:\n",
    "        if key in xds:\n",
    "            new = f\"{key}{level}\"\n",
    "            xds = xds.rename({key: new})\n",
    "            xds[new].attrs[\"long_name\"] = f\"{level} metre \" + xds[new].attrs[\"long_name\"]\n",
    "            xds[new].attrs[\"original_name\"] = key\n",
    "    for key in xds.data_vars:\n",
    "        xds[key].attrs[\"GRIB_level\"] = level\n",
    "        \n",
    "    dsdict[level] = xds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5b9fd0d-2b2b-4bea-979a-240369f9331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdict = {}\n",
    "for xds in dsdict.values():\n",
    "    for varname in sorted(xds.data_vars):\n",
    "        newdict[varname] = {\n",
    "            \"filter_by_keys\": {\n",
    "                \"typeOfLevel\": xds[varname].GRIB_typeOfLevel,\n",
    "                \"paramId\": xds[varname].GRIB_paramId,\n",
    "            },\n",
    "            \"long_name\": xds[varname].long_name,\n",
    "            \"file_suffixes\": xds[varname].attrs[\"file_suffix\"],\n",
    "            \"forecast_only\": varname in (\"tmax2\", \"tmin2\"),\n",
    "        }\n",
    "        if xds[varname].GRIB_typeOfLevel == \"heightAboveGround\":\n",
    "            newdict[varname][\"filter_by_keys\"][\"level\"] = xds[varname].attrs[\"GRIB_level\"]\n",
    "        elif xds[varname].GRIB_typeOfLevel == \"surface\":\n",
    "            newdict[varname][\"filter_by_keys\"][\"stepType\"] = xds[varname].attrs[\"GRIB_stepType\"]\n",
    "        if \"original_name\" in xds[varname].attrs:\n",
    "            newdict[varname][\"original_name\"] = xds[varname].original_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6fcbdb14-10d1-41cc-80ea-ffcf11150027",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdict = {key: newdict[key] for key in sorted(list(newdict.keys()))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3256e969-468b-4a1b-b29b-dde68ed9421e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aptmp2': {'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "   'paramId': 260255,\n",
       "   'level': 2},\n",
       "  'long_name': '2 metre Apparent temperature',\n",
       "  'file_suffixes': [''],\n",
       "  'forecast_only': False,\n",
       "  'original_name': 'aptmp'},\n",
       " 'd2m': {'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "   'paramId': 168,\n",
       "   'level': 2},\n",
       "  'long_name': '2 metre dewpoint temperature',\n",
       "  'file_suffixes': [''],\n",
       "  'forecast_only': False},\n",
       " 'pres80': {'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "   'paramId': 54,\n",
       "   'level': 80},\n",
       "  'long_name': '80 metre Pressure',\n",
       "  'file_suffixes': [''],\n",
       "  'forecast_only': False,\n",
       "  'original_name': 'pres'},\n",
       " 'q80': {'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "   'paramId': 133,\n",
       "   'level': 80},\n",
       "  'long_name': '80 metre Specific humidity',\n",
       "  'file_suffixes': [''],\n",
       "  'forecast_only': False,\n",
       "  'original_name': 'q'},\n",
       " 'r2': {'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "   'paramId': 260242,\n",
       "   'level': 2},\n",
       "  'long_name': '2 metre relative humidity',\n",
       "  'file_suffixes': [''],\n",
       "  'forecast_only': False},\n",
       " 'sh2': {'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "   'paramId': 174096,\n",
       "   'level': 2},\n",
       "  'long_name': '2 metre specific humidity',\n",
       "  'file_suffixes': [''],\n",
       "  'forecast_only': False},\n",
       " 't100': {'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "   'paramId': 130,\n",
       "   'level': 100},\n",
       "  'long_name': '100 metre Temperature',\n",
       "  'file_suffixes': [''],\n",
       "  'forecast_only': False,\n",
       "  'original_name': 't'},\n",
       " 't2m': {'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "   'paramId': 167,\n",
       "   'level': 2},\n",
       "  'long_name': '2 metre temperature',\n",
       "  'file_suffixes': [''],\n",
       "  'forecast_only': False},\n",
       " 't80': {'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "   'paramId': 130,\n",
       "   'level': 80},\n",
       "  'long_name': '80 metre Temperature',\n",
       "  'file_suffixes': [''],\n",
       "  'forecast_only': False,\n",
       "  'original_name': 't'},\n",
       " 'tmax2': {'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "   'paramId': 3015,\n",
       "   'level': 2},\n",
       "  'long_name': '2 metre Maximum temperature',\n",
       "  'file_suffixes': [''],\n",
       "  'forecast_only': True,\n",
       "  'original_name': 'tmax'},\n",
       " 'tmin2': {'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "   'paramId': 3016,\n",
       "   'level': 2},\n",
       "  'long_name': '2 metre Minimum temperature',\n",
       "  'file_suffixes': [''],\n",
       "  'forecast_only': True,\n",
       "  'original_name': 'tmin'},\n",
       " 'u10': {'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "   'paramId': 165,\n",
       "   'level': 10},\n",
       "  'long_name': '10 metre U wind component',\n",
       "  'file_suffixes': [''],\n",
       "  'forecast_only': False},\n",
       " 'u100': {'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "   'paramId': 228246,\n",
       "   'level': 100},\n",
       "  'long_name': '100 metre U wind component',\n",
       "  'file_suffixes': [''],\n",
       "  'forecast_only': False},\n",
       " 'u80': {'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "   'paramId': 131,\n",
       "   'level': 80},\n",
       "  'long_name': '80 metre U component of wind',\n",
       "  'file_suffixes': [''],\n",
       "  'forecast_only': False,\n",
       "  'original_name': 'u'},\n",
       " 'v10': {'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "   'paramId': 166,\n",
       "   'level': 10},\n",
       "  'long_name': '10 metre V wind component',\n",
       "  'file_suffixes': [''],\n",
       "  'forecast_only': False},\n",
       " 'v100': {'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "   'paramId': 228247,\n",
       "   'level': 100},\n",
       "  'long_name': '100 metre V wind component',\n",
       "  'file_suffixes': [''],\n",
       "  'forecast_only': False},\n",
       " 'v80': {'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "   'paramId': 132,\n",
       "   'level': 80},\n",
       "  'long_name': '80 metre V component of wind',\n",
       "  'file_suffixes': [''],\n",
       "  'forecast_only': False,\n",
       "  'original_name': 'v'}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4575d244-4fe7-403e-b5fa-b5c11def0ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36bfd8cf-73a9-41b7-aeb9-104cc3755d5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/tsmith/work/ufs2arco/ufs2arco/sources'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sources.__path__[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0715ef5f-76de-4dde-afb3-7f4f794be8c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{sources.__path__[0]}/reference.gfs.yaml\", \"r\") as f:\n",
    "    reference = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0a76251c-b876-466d-b902-f9b3f68c8ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated = reference.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c5ab0c23-078e-495e-ac81-210fd52b3e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated.update(newdict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "12e0f1f1-7a8a-4589-a72e-5b2630257f56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'filter_by_keys': {'typeOfLevel': 'heightAboveGround',\n",
       "  'paramId': 165,\n",
       "  'level': 10},\n",
       " 'long_name': '10 metre U wind component',\n",
       " 'file_suffixes': [''],\n",
       " 'forecast_only': False}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated[\"u10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c03ef53-8817-4766-bf0b-799c3b1e0ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_suffixes': [''],\n",
       " 'filter_by_keys': {'level': 10,\n",
       "  'paramId': 165,\n",
       "  'typeOfLevel': 'heightAboveGround'},\n",
       " 'forecast_only': False,\n",
       " 'long_name': '10 metre U wind component'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reference[\"u10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c5825341-3687-4cf0-9630-37fec7b3ff42",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated = {key: updated[key] for key in sorted(updated.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "efd0bb53-026a-47e5-8e60-f5585de4e4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reference.gfs.yaml\", \"w\") as f:\n",
    "    yaml.dump(updated, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418864f3-9762-4648-aac3-e5ee48a009c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ufs2arco",
   "language": "python",
   "name": "ufs2arco"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
